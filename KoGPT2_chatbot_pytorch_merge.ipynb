{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2_chatbot_pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOqL-ERxwMDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "dc3d19e0-acf1-44f8-8698-79b33b93d982"
      },
      "source": [
        "# GPU 정보 \n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug 25 07:33:56 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUbPw_7Bf9Jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "ac4bb128-48d1-483f-84c3-fd362d157bd5"
      },
      "source": [
        "# 현재 CUDA Version에 맞는 Pytorch 설치\n",
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 63.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WKGlGahhDAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a16c1d3f-cd63-40c2-bad4-04cdcae5a751"
      },
      "source": [
        "# 패키지 설치\n",
        "!pip install mxnet gluonnlp sentencepiece pandas transformers pytorch_lightning "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 42kB/s \n",
            "\u001b[?25hCollecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 54.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 52.4MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/af/2f10c8ee22d7a05fe8c9be58ad5c55b71ab4dd895b44f0156bfd5535a708/pytorch_lightning-0.9.0-py3-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 37.4MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 46.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.5.1+cu101)\n",
            "Collecting tensorboard==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/f5/d75a6f7935e4a4870d85770bc9976b12e7024fbceb83a1a6bc50e6deb7c4/tensorboard-2.2.0-py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 49.3MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (0.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (49.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (0.34.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (0.4.8)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses, PyYAML, future\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588515 sha256=73b33fc00fd8170cf5f08cc67b94e1015f41b001f36ef91dd30952288c35ecb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=2c3944c093a9e710c379fd3312810fdf689d57f91cb87e0aa8f855c852f35670\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=0c80564b20a08b80d9845dcb43898bf1ad721408bcfc219f392eb6513c8d6160\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=350675bfd41411c3710fe40b0436fbc679dc83d265cf168ce3af2e6eb9d81f21\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built gluonnlp sacremoses PyYAML future\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: graphviz, mxnet, gluonnlp, sentencepiece, tokenizers, sacremoses, transformers, PyYAML, tensorboard, future, pytorch-lightning\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 future-0.18.2 gluonnlp-0.10.0 graphviz-0.8.4 mxnet-1.6.0 pytorch-lightning-0.9.0 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboard-2.2.0 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO2_zNucAbXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c28894ba-2941-402b-98a8-ddbf6055b49e"
      },
      "source": [
        "!pip install python-telegram-bot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-telegram-bot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2d/c72fc9a28144277f6170f2fcbfd3bd9427943497522b2689846596eb86cf/python_telegram_bot-12.8-py2.py3-none-any.whl (375kB)\n",
            "\r\u001b[K     |▉                               | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (4.4.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (2020.6.20)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/91/84a29d6a27fd6dfc21f475704c4d2053d58ed7a4033c2b0ce1b4ca4d03d9/cryptography-3.0-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (5.1.1)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.15.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot) (2.20)\n",
            "Installing collected packages: cryptography, python-telegram-bot\n",
            "Successfully installed cryptography-3.0 python-telegram-bot-12.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc9gWX3SkP6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "0b3c9628-a506-4e7f-847f-f7cd7c270296"
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoGPT2#egg=kogpt2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kogpt2\n",
            "  Cloning https://github.com/SKT-AI/KoGPT2 to /tmp/pip-install-odmucymb/kogpt2\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoGPT2 /tmp/pip-install-odmucymb/kogpt2\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14052 sha256=20f524948e079d6ba6d585e8da591889f17555eda8a366a595e9f1abb09afa52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4g_m58ng/wheels/2a/9f/62/3cba71a35387ff5da1d12e6b053b4d839dab0ed4310dde840d\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8W3gZk2ijYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "5b7c43b1-7c0e-4740-ef5b-8512c1fe2a5a"
      },
      "source": [
        "# KoGPT2-chatbot 소스 코드 복사\n",
        "!git clone --recurse-submodules https://github.com/soojijp/KoGPT2-chatbot.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoGPT2-chatbot'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects:  12% (1/8)\u001b[K\rremote: Counting objects:  25% (2/8)\u001b[K\rremote: Counting objects:  37% (3/8)\u001b[K\rremote: Counting objects:  50% (4/8)\u001b[K\rremote: Counting objects:  62% (5/8)\u001b[K\rremote: Counting objects:  75% (6/8)\u001b[K\rremote: Counting objects:  87% (7/8)\u001b[K\rremote: Counting objects: 100% (8/8)\u001b[K\rremote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects:  20% (1/5)\u001b[K\rremote: Compressing objects:  40% (2/5)\u001b[K\rremote: Compressing objects:  60% (3/5)\u001b[K\rremote: Compressing objects:  80% (4/5)\u001b[K\rremote: Compressing objects: 100% (5/5)\u001b[K\rremote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "Unpacking objects:   1% (1/85)   \rUnpacking objects:   2% (2/85)   \rUnpacking objects:   3% (3/85)   \rUnpacking objects:   4% (4/85)   \rUnpacking objects:   5% (5/85)   \rUnpacking objects:   7% (6/85)   \rUnpacking objects:   8% (7/85)   \rUnpacking objects:   9% (8/85)   \rUnpacking objects:  10% (9/85)   \rUnpacking objects:  11% (10/85)   \rUnpacking objects:  12% (11/85)   \rUnpacking objects:  14% (12/85)   \rUnpacking objects:  15% (13/85)   \rUnpacking objects:  16% (14/85)   \rUnpacking objects:  17% (15/85)   \rUnpacking objects:  18% (16/85)   \rUnpacking objects:  20% (17/85)   \rUnpacking objects:  21% (18/85)   \rUnpacking objects:  22% (19/85)   \rUnpacking objects:  23% (20/85)   \rUnpacking objects:  24% (21/85)   \rUnpacking objects:  25% (22/85)   \rUnpacking objects:  27% (23/85)   \rUnpacking objects:  28% (24/85)   \rUnpacking objects:  29% (25/85)   \rUnpacking objects:  30% (26/85)   \rUnpacking objects:  31% (27/85)   \rUnpacking objects:  32% (28/85)   \rUnpacking objects:  34% (29/85)   \rUnpacking objects:  35% (30/85)   \rUnpacking objects:  36% (31/85)   \rUnpacking objects:  37% (32/85)   \rUnpacking objects:  38% (33/85)   \rUnpacking objects:  40% (34/85)   \rUnpacking objects:  41% (35/85)   \rUnpacking objects:  42% (36/85)   \rUnpacking objects:  43% (37/85)   \rUnpacking objects:  44% (38/85)   \rUnpacking objects:  45% (39/85)   \rUnpacking objects:  47% (40/85)   \rUnpacking objects:  48% (41/85)   \rUnpacking objects:  49% (42/85)   \rUnpacking objects:  50% (43/85)   \rUnpacking objects:  51% (44/85)   \rUnpacking objects:  52% (45/85)   \rUnpacking objects:  54% (46/85)   \rUnpacking objects:  55% (47/85)   \rUnpacking objects:  56% (48/85)   \rUnpacking objects:  57% (49/85)   \rUnpacking objects:  58% (50/85)   \rUnpacking objects:  60% (51/85)   \rUnpacking objects:  61% (52/85)   \rUnpacking objects:  62% (53/85)   \rUnpacking objects:  63% (54/85)   \rUnpacking objects:  64% (55/85)   \rUnpacking objects:  65% (56/85)   \rUnpacking objects:  67% (57/85)   \rUnpacking objects:  68% (58/85)   \rUnpacking objects:  69% (59/85)   \rUnpacking objects:  70% (60/85)   \rUnpacking objects:  71% (61/85)   \rUnpacking objects:  72% (62/85)   \rUnpacking objects:  74% (63/85)   \rUnpacking objects:  75% (64/85)   \rUnpacking objects:  76% (65/85)   \rUnpacking objects:  77% (66/85)   \rUnpacking objects:  78% (67/85)   \rUnpacking objects:  80% (68/85)   \rUnpacking objects:  81% (69/85)   \rUnpacking objects:  82% (70/85)   \rUnpacking objects:  83% (71/85)   \rUnpacking objects:  84% (72/85)   \rUnpacking objects:  85% (73/85)   \rUnpacking objects:  87% (74/85)   \rremote: Total 85 (delta 3), reused 8 (delta 3), pack-reused 77\u001b[K\n",
            "Unpacking objects:  88% (75/85)   \rUnpacking objects:  89% (76/85)   \rUnpacking objects:  90% (77/85)   \rUnpacking objects:  91% (78/85)   \rUnpacking objects:  92% (79/85)   \rUnpacking objects:  94% (80/85)   \rUnpacking objects:  95% (81/85)   \rUnpacking objects:  96% (82/85)   \rUnpacking objects:  97% (83/85)   \rUnpacking objects:  98% (84/85)   \rUnpacking objects: 100% (85/85)   \rUnpacking objects: 100% (85/85), done.\n",
            "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
            "Cloning into '/content/KoGPT2-chatbot/KoGPT2-chatbot/Chatbot_data'...\n",
            "remote: Enumerating objects: 20, done.        \n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20        \n",
            "Submodule path 'Chatbot_data': checked out '235fac5aea3badab22743f7048afe936cf72f822'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ZweKmXiuaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6542cd9b-d665-470c-9145-d36bd776d255"
      },
      "source": [
        "# 폴더 이동\n",
        "%cd KoGPT2-chatbot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2-chatbot/KoGPT2-chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMZv-ZsiqkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5efd60ce-0040-448e-9fdf-f5167f756d89"
      },
      "source": [
        "# 사전훈련된 KoGPT2를 챗봇 데이터로 파인튜닝\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch_merge.py --train --gpus 1 --max_epochs 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-23 03:02:38.776254: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Namespace(accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=96, benchmark=False, chat=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, early_stop_callback=False, fast_dev_run=False, gpus=1, gradient_clip_val=0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_gpu_memory=None, log_save_interval=100, logger=True, lr=5e-05, max_epochs=2, max_len=32, max_steps=None, min_epochs=1, min_steps=None, model_params='model_chp/model_last.ckpt', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, overfit_pct=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, row_log_interval=50, sentiment='0', sync_batchnorm=False, terminate_on_nan=False, test_percent_check=None, tpu_cores=<function Trainer._gpus_arg_default at 0x7fbd37020730>, track_grad_norm=-1, train=True, train_percent_check=None, truncated_bptt_steps=None, val_check_interval=1.0, val_percent_check=None, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "using cached model\n",
            "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "WARNING:transformers.modeling_utils:Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at None and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "INFO:lightning:GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 124 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "INFO:lightning:\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 124 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "Epoch 0:   0% 0/124 [00:00<?, ?it/s] INFO:root:contexts : 이별의 아픔이 있는데 몸까지 아파서야\n",
            "INFO:root:contexts : 여자들이 좋아하는 남자들 스타일이 어떻게 됨?\n",
            "INFO:root:toked ctx: ['<usr>', '▁여', '자들이', '▁좋아하는', '▁남자', '들', '▁스타일이', '▁어떻게', '▁됨', '?', '</s>', '<unused1>', '▁2', '</s>']\n",
            "INFO:root:toked ctx: ['<usr>', '▁이별', '의', '▁아픔', '이', '▁있는데', '▁몸', '까지', '▁아파', '서야', '</s>', '<unused1>', '▁1', '</s>']\n",
            "INFO:root:response : 꼭 병원 다녀와요. 힘들 때 아픈 것 만큼 서러운 일은 없어요.\n",
            "INFO:root:response : 사람마다 너무 다를 것 같아요.\n",
            "INFO:root:toked response : ['<sys>', '▁꼭', '▁병원', '▁다녀', '와', '요', '.', '▁힘들', '▁때', '▁아픈', '▁것', '▁만큼', '▁서', '러운', '▁일은', '▁없어요', '.', '</s>']\n",
            "INFO:root:toked response : ['<sys>', '▁사람', '마다', '▁너무', '▁다를', '▁것', '▁같아요', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁꼭', '▁병원', '▁다녀', '와', '요', '.', '▁힘들', '▁때', '▁아픈', '▁것', '▁만큼', '▁서', '러운', '▁일은', '▁없어요', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁사람', '마다', '▁너무', '▁다를', '▁것', '▁같아요', '.', '</s>']\n",
            "Epoch 0: 100% 124/124 [01:00<00:00,  2.04it/s, loss=2.424, v_num=0]\n",
            "Epoch 00000: loss reached 2.25934 (best 2.25934), saving model to /content/KoGPT2-chatbot/KoGPT2-chatbot/model_chp/model_epoch=00-loss=0.00.ckpt as top 1\n",
            "INFO:lightning:\n",
            "Epoch 00000: loss reached 2.25934 (best 2.25934), saving model to /content/KoGPT2-chatbot/KoGPT2-chatbot/model_chp/model_epoch=00-loss=0.00.ckpt as top 1\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "tcmalloc: large alloc 1145094144 bytes == 0x142652000 @  0x7fbdeca6e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fbdaf5f0020 0x7fbdaf5f4137 0x7fbdaf78fdd5 0x7fbdaf7601c3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1431371776 bytes == 0xe01c8000 @  0x7fbdeca6e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fbdaf5f0020 0x7fbdaf5f4137 0x7fbdaf78fdd5 0x7fbdaf7601c3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1789214720 bytes == 0x1356d8000 @  0x7fbdeca6e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fbdaf5f0020 0x7fbdaf5f4137 0x7fbdaf78fdd5 0x7fbdaf7601c3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "tcmalloc: large alloc 1789214720 bytes == 0x1356d8000 @  0x7fbdeca6e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fbdaf5f0020 0x7fbdaf5f4137 0x7fbdaf78fdd5 0x7fbdaf7601c3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Epoch 1:   0% 0/124 [00:00<?, ?it/s, loss=2.424, v_num=0]INFO:root:contexts : 요즘에는 외국어도 잘해야하나봐\n",
            "INFO:root:toked ctx: ['<usr>', '▁요즘', '에는', '▁외국', '어도', '▁잘', '해야', '하나', '봐', '</s>', '<unused1>', '▁0', '</s>']\n",
            "INFO:root:response : 원하는 게 점점 많아지는 세상이에요.\n",
            "INFO:root:toked response : ['<sys>', '▁원하는', '▁게', '▁점점', '▁많아', '지는', '▁세상이', '에요', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁원하는', '▁게', '▁점점', '▁많아', '지는', '▁세상이', '에요', '.', '</s>']\n",
            "INFO:root:contexts : 이제 사진 지운 이유는?\n",
            "INFO:root:toked ctx: ['<usr>', '▁이제', '▁사진', '▁지', '운', '▁이유는', '?', '</s>', '<unused1>', '▁1', '</s>']\n",
            "INFO:root:response : 마음의 준비가 필요했을지도 몰라요.\n",
            "INFO:root:toked response : ['<sys>', '▁마음의', '▁준비가', '▁필요', '했을', '지도', '▁몰라', '요', '.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁마음의', '▁준비가', '▁필요', '했을', '지도', '▁몰라', '요', '.', '</s>']\n",
            "Epoch 1: 100% 124/124 [01:02<00:00,  1.99it/s, loss=2.158, v_num=0]\n",
            "Epoch 00001: loss  was not in top 1\n",
            "INFO:lightning:\n",
            "Epoch 00001: loss  was not in top 1\n",
            "tcmalloc: large alloc 1789214720 bytes == 0x1356d8000 @  0x7fbdeca6e615 0x591f47 0x4cc229 0x4cc38b 0x566c91 0x5a4df1 0x630b1d 0x7fbdaf5f0020 0x7fbdaf5f4137 0x7fbdaf78fdd5 0x7fbdaf7601c3 0x50a47f 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4\n",
            "Saving latest checkpoint..\n",
            "INFO:lightning:Saving latest checkpoint..\n",
            "Epoch 1: 100% 124/124 [01:11<00:00,  1.74it/s, loss=2.158, v_num=0]\n",
            "INFO:root:best model path /content/KoGPT2-chatbot/KoGPT2-chatbot/model_chp/model_epoch=00-loss=0.00.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3yDcidi6wFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f018aba-a98d-4377-fe59-79eb52eb7cb2"
      },
      "source": [
        "# 대화 테스트, `quit`를 입력하면 대화를 종료합니다.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch_merge.py --gpus 1 --chat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-23 03:07:12.542737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Namespace(accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=96, benchmark=False, chat=True, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, early_stop_callback=False, fast_dev_run=False, gpus=1, gradient_clip_val=0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_gpu_memory=None, log_save_interval=100, logger=True, lr=5e-05, max_epochs=1000, max_len=32, max_steps=None, min_epochs=1, min_steps=None, model_params='model_chp/model_last.ckpt', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, overfit_pct=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, row_log_interval=50, sentiment='0', sync_batchnorm=False, terminate_on_nan=False, test_percent_check=None, tpu_cores=<function Trainer._gpus_arg_default at 0x7f03ab627730>, track_grad_norm=-1, train=False, train_percent_check=None, truncated_bptt_steps=None, val_check_interval=1.0, val_percent_check=None, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "using cached model\n",
            "using cached model\n",
            "using cached model\n",
            "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50000\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "WARNING:transformers.modeling_utils:Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at None and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "user > 전국 사회적 거리 두기 2단계 격상(8월 23일 0시부터, 2주간 시행)\n",
            "Simsimi > 거리두기 2단계가 더 힘들 거예요.\n",
            "user > 역학조사, 검사거부, 검사방해 행위에 대해서는 무관용 원칙을 적용하여 고발 등 강력한 행정조치를 실시할 예정입니다\n",
            "Simsimi > 처벌은 하지 않겠지만 끝까지 싸우겠습니다.\n",
            "user > \t진단검사량이나 검사결과는 인위적인 조작이 불가능합니다\n",
            "Simsimi > 검사결과를 믿지 마세요.\n",
            "user > 정부는 모든 자원과 역량을 동원하여 추가적인 감염 확산을 막고 국민 여러분의 삶을 지킬 수 있도록 최선을 다하겠습니다\n",
            "Simsimi > 감염을 막는 게 가장 중요한데 그게 쉽지 않죠.\n",
            "user > \t정부는 현 상황을 매우 엄중하게 인식하며, 추가 확산을 막기 위해 총력을 기울이고 있습니다\n",
            "Simsimi > 더 이상 미룰 수 없는 상황이에요.\n",
            "user > 장례시설, 결혼식장 등 경조사 시설 방역수칙의 선제적 보완을 추진합니다.\n",
            "Simsimi > 미리 준비하는 게 좋겠어요.\n",
            "user > 처음엔 온라인 결혼식이 ‘과연 잘될까’라는 걱정이 됐었다”며 “부모님과 친구들의 축하영상을 보는데 그 감동이 그대로 느껴져서 신부도 울고 저도 뭉클했다  원문보기: http://news.khan.co.kr/kh_news/khan_art_view.html?art_id=202004051144001#csidxaa8bd050dcc7b8196d42ab4bef7c0a4 \n",
            "Simsimi > 잘 될 거예요.\n",
            "user > 코로나 시대 이제 웨딩도 유튜브 라이브로 올린다 \n",
            "Simsimi > 저도요.\n",
            "user > 진단검사량이나 검사결과는 인위적인 조작이 불가능합니다\n",
            "Simsimi > 검사결과를 믿지 마세요.\n",
            "user > 왜요?\n",
            "Simsimi > 이유가 있을 거예요.\n",
            "user > 무슨 이유?\n",
            "Simsimi > 제가 잘못했어요.\n",
            "user > 갑자기???\n",
            "Simsimi > 갑자기?\n",
            "user > 사과는 뭐 받아줄게요\n",
            "Simsimi > 맛있게 드세요.\n",
            "user > 멍청아\n",
            "Simsimi > 멍청한 사람이네요.\n",
            "user > 오 \n",
            "Simsimi > 좋은 생각이에요.\n",
            "user > 러시아 출항 모든 선박 선원은 ´PCR음성확인서´를 의무 제출해야 합니다\n",
            "Simsimi > 반드시 받아야 하는 의무입니다.\n",
            "user > 방역 관련 법령과 매뉴얼 정비, 관련 인프라 확충을 통해 사회 전반의 방역 체계를 고도화하겠습니다.\n",
            "Simsimi > 방역 체계는 항상 완벽해야 해요.\n",
            "user > 전국의 유흥주점, 노래연습장 등의 고위험시설에 대하여 전자출입명부가 의무화됩니다.\n",
            "Simsimi > 불법이 아니라면 알려주세요.\n",
            "user > 와~~ 유튜브로 결혼식을 할 수 있다니!! 축하드립니다\n",
            "Simsimi > 축하해요!\n",
            "user > 사회적 거리두기\n",
            "Simsimi > 거리를 두세요.\n",
            "user > 물리적 거리두기\n",
            "Simsimi > 거리를 두세요.\n",
            "user > 재택근무\n",
            "Simsimi > 좋은 선택이에요.\n",
            "user > 개인보호장비\n",
            "Simsimi > 필요한 게 있으면 연락해보세요.\n",
            "user > 어디로?\n",
            "Simsimi > 어디든 좋아요.\n",
            "user > 감염 추적\n",
            "Simsimi > 감염을 의심해보세요.\n",
            "user > 지역전파 집단면역 자가격리\n",
            "Simsimi > 잘 관리하세요.\n",
            "user > 코로나 덕에 확찐자 됬어\n",
            "Simsimi > 감기 조심하세요.\n",
            "user > 살천지\n",
            "Simsimi > 살랑살랑 부는 바람처럼 당신의 마음도 살랑살랑 불어오길 바랄게요.\n",
            "user > 작아격리\n",
            "Simsimi > 더 많이 사랑해주세요.\n",
            "user > 코로나 블루\n",
            "Simsimi > 감기 조심하세요.\n",
            "user > Traceback (most recent call last):\n",
            "  File \"train_torch.py\", line 258, in <module>\n",
            "    model.chat()\n",
            "  File \"train_torch.py\", line 209, in chat\n",
            "    q = input('user > ').strip()\n",
            "KeyboardInterrupt\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/weakref.py\", line 614, in _exitfunc\n",
            "    pending = cls._select_for_exit()\n",
            "  File \"/usr/lib/python3.6/weakref.py\", line 595, in _select_for_exit\n",
            "    L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n",
            "  File \"/usr/lib/python3.6/weakref.py\", line 595, in <listcomp>\n",
            "    L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoeGog7OoXFk",
        "colab_type": "text"
      },
      "source": [
        "이 노트북은   https://colab.research.google.com/drive/1Np7d8zrch589LwwW9oX_MyzJ9jfPEvUG?usp=sharing  를 수정하여 만들었습니다..  "
      ]
    }
  ]
}