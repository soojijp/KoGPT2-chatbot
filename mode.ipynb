{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kogpt2.mxnet_kogpt2 import get_mxnet_kogpt2_model\n",
    "from kogpt2.utils import get_tokenizer\n",
    "from mxnet import gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Simsimi based on KoGPT-2')\n",
    "\n",
    "parser.add_argument('--num-epoch',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help='number of iterations to train (default: 2)')\n",
    "\n",
    "parser.add_argument('--max-seq-len',\n",
    "                    type=int,\n",
    "                    default=32,\n",
    "                    help='max sentence length on input (default: 32)')\n",
    "\n",
    "parser.add_argument('--batch-size',\n",
    "                    type=int,\n",
    "                    default=64,\n",
    "                    help='batch size for training (default: 64)')\n",
    "\n",
    "parser.add_argument('--chat',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='response generation on given user input')\n",
    "\n",
    "parser.add_argument('--sentiment',\n",
    "                    type=str,\n",
    "                    default='0',\n",
    "                    help='sentiment for system. 0 is neutral, 1 is negative, 2 is positive.')\n",
    "\n",
    "\n",
    "parser.add_argument('--model_params',\n",
    "                    type=str,\n",
    "                    default='kogpt2_chat.params',\n",
    "                    help='model binary for starting chat')\n",
    "\n",
    "parser.add_argument('--train',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='eval train set (default: False)')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    '--accumulate',\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\n",
    "    'accumulate gradient to achieve the same result with a large batch size')\n",
    "\n",
    "opt = parser.parse_args() #1\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "U_TKN = '<usr>'\n",
    "S_TKN = '<sys>'\n",
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<unused0>'\n",
    "SENT = '<unused1>'\n",
    "\n",
    "\n",
    "class chat_data(gluon.data.Dataset):\n",
    "    def __init__(self, chats, tok_path, vocab, max_len=32):\n",
    "        self._data = chats\n",
    "        self._tok_path = tok_path\n",
    "        self.tokenizer = None\n",
    "        self.first = True\n",
    "        self.q_token = U_TKN\n",
    "        self.a_token = S_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.bos = BOS\n",
    "        self.eos = EOS\n",
    "        self.maskt = MASK\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.padder = nlp.data.PadSequence(\n",
    "            max_len, pad_val=self.vocab[self.vocab.padding_token])\n",
    "\n",
    "    def _activate_sp(self):\n",
    "        self.tokenizer = nlp.data.SentencepieceTokenizer(self._tok_path, 0, 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.tokenizer is None:\n",
    "            self._activate_sp()\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn['Q']\n",
    "        a = turn['A']\n",
    "        sentiment = str(turn['label'])\n",
    "        q_toked = [\n",
    "            self.q_token,\n",
    "        ] + self.tokenizer(q) + [\n",
    "            self.eos,\n",
    "        ] + [self.sent_token] + self.tokenizer(sentiment) + [\n",
    "            self.eos,\n",
    "        ]\n",
    "        q_len = len(q_toked)\n",
    "        a_toked = [\n",
    "            self.a_token,\n",
    "        ] + self.tokenizer(a) + [\n",
    "            self.eos,\n",
    "        ]\n",
    "        a_len = len(a_toked)\n",
    "        if q_len + a_len > self.max_len:\n",
    "            remains = self.max_len - q_len\n",
    "            a_len = remains\n",
    "            a_toked = a_toked[-a_len:]\n",
    "            assert a_len == len(a_toked)\n",
    "        # [mask, mask, ...., mask, ..., <bos>,..A.. <eos>, <pad>....]\n",
    "        labels = [\n",
    "            self.maskt,\n",
    "        ] * q_len + a_toked[1:]\n",
    "        if self.first:\n",
    "            logging.info(\"contexts : {}\".format(q))\n",
    "            logging.info(\"toked ctx: {}\".format(q_toked))\n",
    "            logging.info(\"response : {}\".format(a))\n",
    "            logging.info(\"toked response : {}\".format(a_toked))\n",
    "            logging.info('labels {}'.format(labels))\n",
    "            self.first = False\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        return (self.padder(self.vocab[q_toked + a_toked]), nd.array(mask),\n",
    "                self.padder(self.vocab[labels]))\n",
    "\n",
    "\n",
    "class KoGPT2Chat(nn.HybridBlock):\n",
    "    def __init__(self, kogpt2, prefix=None, params=None):\n",
    "        super(KoGPT2Chat, self).__init__(prefix=prefix, params=params)\n",
    "        self.kogpt2 = kogpt2\n",
    "\n",
    "    def hybrid_forward(self, F, inputs):\n",
    "        # (batch, seq_len, hiddens)\n",
    "        output, _ = self.kogpt2(inputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "if mx.context.num_gpus() > 0:\n",
    "    ctx = mx.gpu()\n",
    "else:\n",
    "    ctx = mx.cpu()\n",
    "\n",
    "\n",
    "def train():\n",
    "    tok_path = get_tokenizer()\n",
    "    model, vocab = get_mxnet_kogpt2_model(ctx=ctx)\n",
    "    # tok = SentencepieceTokenizer(tok_path, num_best=0, alpha=0)\n",
    "\n",
    "    data = pd.read_csv('Chatbot_data/ChatbotData.csv')\n",
    "\n",
    "    max_len = opt.max_seq_len\n",
    "    train_set = chat_data(data, tok_path, vocab, max_len=max_len)\n",
    "    batch_size = opt.batch_size\n",
    "\n",
    "    train_dataloader = mx.gluon.data.DataLoader(train_set,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=5,\n",
    "                                                shuffle=True)\n",
    "    kogptqa = KoGPT2Chat(model)\n",
    "    kogptqa.hybridize()\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss_function.hybridize()\n",
    "\n",
    "    num_epochs = opt.num_epoch\n",
    "    lr = 5e-5\n",
    "    trainer = gluon.Trainer(kogptqa.collect_params(), 'bertadam', {\n",
    "        'learning_rate': lr,\n",
    "        'epsilon': 1e-8,\n",
    "        'wd': 0.01\n",
    "    })\n",
    "    # LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다.\n",
    "    for _, v in kogptqa.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "        v.wd_mult = 0.0\n",
    "    params = [\n",
    "        p for p in kogptqa.collect_params().values() if p.grad_req != 'null'\n",
    "    ]\n",
    "    # learning rate warmup\n",
    "    accumulate = opt.accumulate\n",
    "    step_size = batch_size * accumulate if accumulate else batch_size\n",
    "    num_train_examples = len(train_set)\n",
    "    num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "    warmup_ratio = 0.1\n",
    "    num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "    step_num = 0\n",
    "    all_model_params = kogptqa.collect_params()\n",
    "\n",
    "    log_interval = 50\n",
    "    neg = -1e18\n",
    "    # Set grad_req if gradient accumulation is required\n",
    "    if accumulate and accumulate > 1:\n",
    "        for p in params:\n",
    "            p.grad_req = 'add'\n",
    "\n",
    "    for epoch_id in range(num_epochs):\n",
    "        step_loss = 0\n",
    "        for batch_id, (token_ids, mask, label) in enumerate(train_dataloader):\n",
    "            if step_num < num_warmup_steps:\n",
    "                new_lr = lr * step_num / num_warmup_steps\n",
    "            else:\n",
    "                non_warmup_steps = step_num - num_warmup_steps\n",
    "                offset = non_warmup_steps / (num_train_steps -\n",
    "                                             num_warmup_steps)\n",
    "                new_lr = lr - offset * lr\n",
    "            trainer.set_learning_rate(new_lr)\n",
    "            with mx.autograd.record():\n",
    "                # load data to GPU or GPU\n",
    "                token_ids = token_ids.as_in_context(ctx)\n",
    "                mask = mask.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                # forward computation\n",
    "                out = kogptqa(token_ids)\n",
    "                masked_out = nd.where(\n",
    "                    mask.expand_dims(axis=2).repeat(repeats=out.shape[2],\n",
    "                                                    axis=2), out,\n",
    "                    neg * nd.ones_like(out))\n",
    "                # loss for responses exincluding MASK and PAD\n",
    "                ls = loss_function(masked_out, label).sum() / mask.sum()\n",
    "            # backward computation\n",
    "            ls.backward()\n",
    "            if not accumulate or (batch_id + 1) % accumulate == 0:\n",
    "                trainer.allreduce_grads()\n",
    "                nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                trainer.update(accumulate if accumulate else 1)\n",
    "                step_num += 1\n",
    "                if accumulate and accumulate > 1:\n",
    "                    # set grad to zero for gradient accumulation\n",
    "                    all_model_params.zero_grad()\n",
    "            step_loss += ls.asscalar()\n",
    "            if step_num % log_interval == 0 and step_num > 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, train ppl={:.3f}'\n",
    "                    .format(epoch_id + 1, batch_id + 1, len(train_dataloader),\n",
    "                            step_loss / log_interval, trainer.learning_rate,\n",
    "                            math.exp(step_loss / log_interval)))\n",
    "                step_loss = 0\n",
    "    logging.info('saving model file to {}'.format(opt.model_params))\n",
    "    kogptqa.save_parameters(opt.model_params)\n",
    "\n",
    "\n",
    "def chat(model_params, sent='0'):\n",
    "    tok_path = get_tokenizer()\n",
    "    model, vocab = get_mxnet_kogpt2_model(ctx=ctx)\n",
    "    tok = SentencepieceTokenizer(tok_path, num_best=0, alpha=0)\n",
    "    kogptqa = KoGPT2Chat(model)\n",
    "    kogptqa.load_parameters(model_params, ctx=ctx)\n",
    "    sent_tokens = tok(sent)\n",
    "    while 1:\n",
    "        q = input('user > ').strip()\n",
    "        if q == 'quit':\n",
    "            break\n",
    "        q_tok = tok(q)\n",
    "        a = ''\n",
    "        a_tok = []\n",
    "        while 1:\n",
    "            input_ids = mx.nd.array([vocab[U_TKN]] + vocab[q_tok] +\n",
    "                                    vocab[EOS, SENT] + vocab[sent_tokens] + \n",
    "                                    vocab[EOS, S_TKN] +\n",
    "                                    vocab[a_tok]).expand_dims(axis=0)\n",
    "            pred = kogptqa(input_ids.as_in_context(ctx))\n",
    "            gen = vocab.to_tokens(\n",
    "                mx.nd.argmax(\n",
    "                    pred,\n",
    "                    axis=-1).squeeze().astype('int').asnumpy().tolist())[-1]\n",
    "            if gen == EOS:\n",
    "                break\n",
    "            a += gen.replace('▁', ' ')\n",
    "            a_tok = tok(a)\n",
    "        print(\"Simsimi > {}\".format(a.strip()))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if opt.train:\n",
    "        train()\n",
    "    if opt.chat:\n",
    "        chat(opt.model_params, opt.sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
